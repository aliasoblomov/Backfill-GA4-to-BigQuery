{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliasoblomov/Backfill-GA4-to-BigQuery/blob/main/GA4_tables_backfill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WJNl6xTYm4_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install google-analytics-data==0.18.4\n",
        "!pip install google-cloud-bigquery\n",
        "!pip install google-auth==2.27.0\n",
        "!pip install google-auth-oauthlib\n",
        "!pip install google-auth-httplib2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='transactionId'),\n",
        "            Dimension(name='itemName'),\n",
        "            Dimension(name='date')  # Added 'date' dimension\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='itemPurchaseQuantity'),\n",
        "            Metric(name='itemRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        transaction_id = row.dimension_values[0].value\n",
        "        item_name = row.dimension_values[1].value\n",
        "        date_value = row.dimension_values[2].value  # Added date handling\n",
        "        list_rows.append({\n",
        "            'transactionId': transaction_id,\n",
        "            'itemName': item_name,\n",
        "            'date': date_value,  # Added date column\n",
        "            'itemPurchaseQuantity': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'itemRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"transactionId\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"itemName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"date\", \"STRING\"),  # Added date field in schema\n",
        "        bigquery.SchemaField(\"itemPurchaseQuantity\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"itemRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'transactionId' field\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_transaction_items'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "003OzBhNUl7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'), Dimension(name='sessionDefaultChannelGroup')],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        session_channel_group = row.dimension_values[1].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'sessionPrimaryChannelGroup': session_channel_group,\n",
        "            'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessionPrimaryChannelGroup\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_session_channel_group'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "TaCbme6LYqD4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='sessionSource'),\n",
        "            Dimension(name='sessionCampaignName'),\n",
        "            Dimension(name='sessionMedium')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        session_source = row.dimension_values[1].value\n",
        "        session_campaign_name = row.dimension_values[2].value\n",
        "        session_medium = row.dimension_values[3].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'sessionSource': session_source,\n",
        "            'sessionCampaignName': session_campaign_name,\n",
        "            'sessionMedium': session_medium,\n",
        "            'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessionSource\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionCampaignName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionMedium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_session_source_campaign_medium'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Wz5wF6MHbIAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='country'),\n",
        "            Dimension(name='language'),\n",
        "            Dimension(name='city')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        country = row.dimension_values[1].value\n",
        "        language = row.dimension_values[2].value\n",
        "        city = row.dimension_values[3].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'country': country,\n",
        "            'language': language,\n",
        "            'city': city,\n",
        "            'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'screenPageViews': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[5].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"country\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"language\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"city\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_country_language_city'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "e-Oqh-oNfbC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=1000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='itemName')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='itemPurchaseQuantity'),\n",
        "            Metric(name='itemRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        item_name = row.dimension_values[1].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'itemName': item_name,\n",
        "            'itemPurchaseQuantity': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'itemRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"itemName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"itemPurchaseQuantity\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"itemRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_item_name'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RKU2hiP7gynQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=1000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='browser'),\n",
        "            Dimension(name='operatingSystem'),\n",
        "            Dimension(name='deviceCategory')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        browser = row.dimension_values[1].value\n",
        "        operating_system = row.dimension_values[2].value\n",
        "        device_category = row.dimension_values[3].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'browser': browser,\n",
        "            'operatingSystem': operating_system,\n",
        "            'deviceCategory': device_category,\n",
        "            'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'screenPageViews': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[5].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"browser\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"operatingSystem\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"deviceCategory\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_browser_os_device'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YpYm_kTLiqsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='firstUserMedium'),\n",
        "            Dimension(name='firstUserSource'),\n",
        "            Dimension(name='firstUserCampaignName')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        first_user_medium = row.dimension_values[1].value\n",
        "        first_user_source = row.dimension_values[2].value\n",
        "        first_user_campaign_name = row.dimension_values[3].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'firstUserMedium': first_user_medium,\n",
        "            'firstUserSource': first_user_source,\n",
        "            'firstUserCampaignName': first_user_campaign_name,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"firstUserMedium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"firstUserSource\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"firstUserCampaignName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_first_user_source_medium'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "s5H79ndims88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='firstUserDefaultChannelGroup')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        first_user_channel_group = row.dimension_values[1].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'firstUserDefaultChannelGroup': first_user_channel_group,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"firstUserDefaultChannelGroup\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_data_first_user_channel_group'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Fv1ISKAUn-bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='sessionSource'),\n",
        "            Dimension(name='sessionMedium'),\n",
        "            Dimension(name='sessionCampaignName')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='averagePurchaseRevenue'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "            Metric(name='advertiserAdClicks'),\n",
        "            Metric(name='advertiserAdCost'),\n",
        "            Metric(name='advertiserAdCostPerClick'),\n",
        "            Metric(name='returnOnAdSpend')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        session_source = row.dimension_values[1].value\n",
        "        session_medium = row.dimension_values[2].value\n",
        "        session_campaign_name = row.dimension_values[3].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'sessionSource': session_source,\n",
        "            'sessionMedium': session_medium,\n",
        "            'sessionCampaignName': session_campaign_name,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'averagePurchaseRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'advertiserAdClicks': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'advertiserAdCost': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0,\n",
        "            'advertiserAdCostPerClick': pd.to_numeric(row.metric_values[5].value, errors='coerce') or 0,\n",
        "            'returnOnAdSpend': pd.to_numeric(row.metric_values[6].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessionSource\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionMedium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionCampaignName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"averagePurchaseRevenue\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"advertiserAdClicks\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"advertiserAdCost\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"advertiserAdCostPerClick\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"returnOnAdSpend\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_ads_data'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "TB_tq1b0rvkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='transactionId'),\n",
        "            Dimension(name='itemName')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='itemPurchaseQuantity'),\n",
        "            Metric(name='itemRevenue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        transaction_id = row.dimension_values[0].value\n",
        "        item_name = row.dimension_values[1].value\n",
        "        list_rows.append({\n",
        "            'transactionId': transaction_id,\n",
        "            'itemName': item_name,\n",
        "            'itemPurchaseQuantity': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'itemRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"transactionId\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"itemName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"itemPurchaseQuantity\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"itemRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'transactionId' field\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_transaction_items'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Dt6n-vqwt1NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date')],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='eventCount'),\n",
        "            Metric(name='averageSessionDuration'),\n",
        "            Metric(name='engagedSessions'),\n",
        "            Metric(name='engagementRate')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'newUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0,\n",
        "            'screenPageViews': pd.to_numeric(row.metric_values[5].value, errors='coerce') or 0,\n",
        "            'eventCount': pd.to_numeric(row.metric_values[6].value, errors='coerce') or 0,\n",
        "            'averageSessionDuration': pd.to_numeric(row.metric_values[7].value, errors='coerce') or 0,\n",
        "            'engagedSessions': pd.to_numeric(row.metric_values[8].value, errors='coerce') or 0,\n",
        "            'engagementRate': pd.to_numeric(row.metric_values[9].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"eventCount\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"averageSessionDuration\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"engagedSessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"engagementRate\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_all_metrics_data'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Wb8umpjezZsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=10000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='eventName')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='eventCount'),\n",
        "            Metric(name='eventCountPerUser'),\n",
        "            Metric(name='eventValue')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        event_name = row.dimension_values[1].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'eventName': event_name,\n",
        "            'eventCount': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'eventCountPerUser': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'eventValue': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"eventName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"eventCount\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"eventCountPerUser\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"eventValue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_event_metrics_data'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "CgQ4MPAf1A_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=250000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='pageLocation')  # New dimension\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='eventCount'),\n",
        "            Metric(name='engagementRate')  # New metrics\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        page_location = row.dimension_values[1].value  # New dimension\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'pageLocation': page_location,  # New dimension\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'screenPageViews': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'eventCount': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0,\n",
        "            'engagementRate': pd.to_numeric(row.metric_values[5].value, errors='coerce') or 0  # New metric\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"pageLocation\", \"STRING\"),  # New dimension\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),  # New metric\n",
        "        bigquery.SchemaField(\"eventCount\", \"INTEGER\"),  # New metric\n",
        "        bigquery.SchemaField(\"engagementRate\", \"FLOAT\")  # New metric\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_page_location_data'  # New table name\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "dTobTzk1nDNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import Flow\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Authenticate with service account for BigQuery\n",
        "creds1 = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'],\n",
        "    scopes=['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        ")\n",
        "bq_client = bigquery.Client(credentials=creds1, project=creds1.project_id)\n",
        "\n",
        "# Authenticate for GA4 Analytics Data API using OAuth2\n",
        "def authenticate_ga4():\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    else:\n",
        "        flow = Flow.from_client_secrets_file(\n",
        "            config['CLIENT_SECRET_FILE'],\n",
        "            scopes=config['SCOPES'],\n",
        "            redirect_uri='http://localhost:8080/'\n",
        "        )\n",
        "        auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "        print('Please go to this URL and finish the authentication: ', auth_url)\n",
        "        code = input('Enter the authorization code: ')\n",
        "        flow.fetch_token(code=code)\n",
        "        creds = flow.credentials\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    return creds\n",
        "\n",
        "# Function to paginate and fetch GA4 report data with logging\n",
        "def run_report_with_pagination(client, request, limit=250000):\n",
        "    all_rows = []\n",
        "    offset = 0\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Apply offset and limit to request\n",
        "        request.offset = offset\n",
        "        request.limit = limit\n",
        "\n",
        "        # Fetch report data\n",
        "        response = client.run_report(request)\n",
        "        all_rows.extend(response.rows)\n",
        "\n",
        "        print(f\"Fetching data... Page {page_number}, Offset: {offset}, Rows fetched: {len(response.rows)}\")\n",
        "\n",
        "        # If fewer rows are fetched than the limit, we're done\n",
        "        if len(response.rows) < limit:\n",
        "            break\n",
        "\n",
        "        # Update offset and page number to get the next set of rows\n",
        "        offset += limit\n",
        "        page_number += 1\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# Function to fetch GA4 data using pagination\n",
        "def get_ga4_report(client):\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[\n",
        "            Dimension(name='date'),\n",
        "            Dimension(name='landingPage')\n",
        "        ],\n",
        "        metrics=[\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='eventCount'),\n",
        "            Metric(name='engagementRate')\n",
        "        ]\n",
        "    )\n",
        "    return run_report_with_pagination(client, request)\n",
        "\n",
        "# Function to convert GA4 response to a DataFrame\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "    for row in response:\n",
        "        try:\n",
        "            date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "        except ValueError:\n",
        "            date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "        landing_page = row.dimension_values[1].value\n",
        "        list_rows.append({\n",
        "            'date': date_value,\n",
        "            'landingPage': landing_page,\n",
        "            'totalUsers': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "            'ecommercePurchases': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "            'purchaseRevenue': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "            'sessions': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "            'eventCount': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0,\n",
        "            'engagementRate': pd.to_numeric(row.metric_values[5].value, errors='coerce') or 0\n",
        "        })\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "# Function to upload data to BigQuery\n",
        "def upload_to_bigquery(df, table_id):\n",
        "    # Define BigQuery schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"landingPage\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"eventCount\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"engagementRate\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{bq_client.project}.{config['DATASET_ID']}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload the DataFrame to BigQuery\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Authenticate GA4 using OAuth2\n",
        "        creds = authenticate_ga4()\n",
        "        client_ga4 = BetaAnalyticsDataClient(credentials=creds)\n",
        "\n",
        "        # Fetch GA4 data\n",
        "        ga4_response = get_ga4_report(client_ga4)\n",
        "\n",
        "        # Convert the response to a DataFrame\n",
        "        ga4_df = response_to_dataframe(ga4_response)\n",
        "\n",
        "        # Define the BigQuery table ID and CSV filename (same as table ID)\n",
        "        table_id = 'ga4_landing_page_data'\n",
        "        csv_filename = f\"{table_id}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        ga4_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "        # Upload the DataFrame to BigQuery\n",
        "        upload_to_bigquery(ga4_df, table_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "T5zaPVqat6kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}